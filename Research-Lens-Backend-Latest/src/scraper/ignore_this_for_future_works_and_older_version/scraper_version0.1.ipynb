{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf22019-cfad-4469-a20c-2fa9956f8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd1f0d8d-d61b-479b-b81f-210ffa443372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting user inputs\n",
    "query_input   = input(\"\\nEnter your search term for arXiv: \").strip() #what you need to search in arxiv\n",
    "choice_papers = int(input(\"\\nEnter how many Papers per page (can only input number [25,50,100,200] only if not default value is 50): \").strip())\n",
    "num_pages     = int(input(\"\\nEnter how many pages to scrape (each has above mentioned papers): \").strip())\n",
    "choice        = input(\"\\nEnter the choice for paper sorting\\n\"\n",
    "                        \"1. Latest by announce date\\n\"\n",
    "                        \"2. Latest by submition date\\n\"\n",
    "                        \"3. Oldest by announce date\\n\"\n",
    "                        \"4. Oldest by submition date\\n\"\n",
    "                        \"5. By Relevence\\n\"\n",
    "                        \"   Default option is '1'\\n\"\n",
    "                        \"Entered Choice >>>\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7151038d-52fe-4b27-bc34-7c7d3e8764b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting_order setting(selection) based on choice entered\n",
    "if choice == \"1\":\n",
    "    order = \"-announced_date_first\"\n",
    "elif choice == \"2\":\n",
    "    order = \"-submitted_date\"\n",
    "elif choice == \"3\":\n",
    "    order = \"announced_date_first\"    \n",
    "elif choice == \"4\":\n",
    "    order = \"submitted_date\"\n",
    "elif choice == \"5\":\n",
    "    order = \"\"\n",
    "else:\n",
    "    order = \"-announced_date_first\"  # default value\n",
    "\n",
    "#Setting the papers(results) per page according to user input choice_papers\n",
    "if choice_papers in [25, 50, 100 , 200 ]:\n",
    "    num_papers = choice_papers\n",
    "else:\n",
    "    num_papers = 50 #default number of results(papers) per page    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d87a857-bf11-4a49-b144-c84731301d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 50 Papers...\n",
      "from 1 pages and per page 50 papers\n",
      "\n",
      "Related to 'architecture security'\n",
      "\n",
      "Taking papers based on the sorting : -submitted_date\n"
     ]
    }
   ],
   "source": [
    "#Just printing what we are going to scrape and how many \n",
    "total_papers=num_papers*num_pages\n",
    "\n",
    "print(f\"Scraping {total_papers} Papers...\\nfrom {num_pages} pages and per page {num_papers} papers\\n\")\n",
    "print(f\"Related to '{query_input}'\\n\")\n",
    "print(f\"Taking papers based on the sorting : {order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9675f62d-864a-4734-906a-5be6fe30ee53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/search/?query=architecture+security&searchtype=all&abstracts=show&order=-submitted_date&size=50'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Url creation according to arxiv\n",
    "query = query_input.replace(\" \", \"+\")\n",
    "base_url = \"https://arxiv.org/search/\"\n",
    "url_temp = f\"{base_url}?query={query}&searchtype=all&abstracts=show&order={order}&size={num_papers}\"\n",
    "\n",
    "#showing how the url would look like\n",
    "url_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d51139b-bc81-41e4-adb4-d2ec1e686bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging - Getting the http request\n",
    "response = requests.get(url_temp)\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbcebaab-ba8a-4312-82d8-01d1843dc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugiing\n",
    "#a=\"&start=100\"\n",
    "#x=f\"{url}{a}\"\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd074d32-16f1-4039-bc8e-7f18a561db27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f6f703-4ba7-43b2-8452-4a2108a551ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fetching page number 1\n",
      "https://arxiv.org/search/?query=architecture+security&searchtype=all&abstracts=show&order=-submitted_date&size=50&start=0\n",
      "Paper number: \n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, \n",
      "\n",
      "\n",
      "Completed fetching data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "papers=[] #array to store the scraped papers\n",
    "count=0 #for counting how many papers fetched\n",
    "for page in range(num_pages):\n",
    "    start0 = page*num_papers\n",
    "    start1 =f\"&start={start0}\"\n",
    "    url = f\"{url_temp}{start1}\"\n",
    "    \n",
    "    print(f\"\\n\\nFetching page number {page+1}\\n{url}\")\n",
    "    print(\"Paper number: \")\n",
    "   \n",
    "    response = requests.get(url) #getting the http request\n",
    "    response.raise_for_status() #if http request failed show error message\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\") #creating soup object for futher extracion\n",
    "\n",
    "    for item in soup.select(\"li.arxiv-result\"): #\"li.arxiv-result\"  is the tag that for each paper section so this will iterate over all in current page\n",
    "        #extracting title of paper\n",
    "        title_tag = item.select_one(\"p.title\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"No Title Found\"\n",
    "\n",
    "        #extract publication info, date, .... \n",
    "        meta_tag = item.select_one(\"p.is-size-7\")\n",
    "        meta_text = meta_tag.get_text(\" \", strip=True) if meta_tag else \"No metadata found\"\n",
    "\n",
    "        #extracting Abstract\n",
    "        abstract_tag = item.select_one(\"span.abstract-full\")\n",
    "        abstract = abstract_tag.get_text(strip=True) if abstract_tag else \"No Abstract Found\"\n",
    "\n",
    "        #debugging \n",
    "        count=count+1\n",
    "        print(f\"{count}, \",end=\"\")\n",
    "        \n",
    "        #saving each in to papers\n",
    "        papers.append((title, meta_text, abstract))\n",
    "        time.sleep(1/50) #just adding some time delay for each request to not to spam the server kinda\n",
    "\n",
    "        \n",
    "print(\"\\n\\n\\nCompleted fetching data\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cedc4144-e895-40de-97d1-5f33558b3ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 50 papers fetched\n"
     ]
    }
   ],
   "source": [
    "print(f\"Totally {len(papers)} papers fetched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd440d85-dafc-4d27-b297-9bc72f22cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = query_input.replace(\"/\", \"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a562ea2-9b58-4742-a8ca-a8e44e67f1a5",
   "metadata": {},
   "source": [
    "for future note\n",
    "need to specify path etc and what file type does the data should be stored etc...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "696a1957-f3d3-4c46-97ba-c15e4888a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Scraping 50 newest papers for  'architecture security' .\n"
     ]
    }
   ],
   "source": [
    "#storing the result to a file inside folder named arxivscrpped . (here now to just a text file that uses utf-8 encoding)\n",
    "folder_name = \"arxivscraped\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "filename = f\"arxiv_{file_name}_{total_papers}_{order}.txt\" #deciding file name\n",
    "filepath = f\"{folder_name}/{filename}\" #deciding path\n",
    "\n",
    "with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, (title,meta_text, abstract) in enumerate(papers, 1):\n",
    "        f.write(f\"Paper {idx}:\\n\")\n",
    "        f.write(f\"Title   : {title}\\n\")\n",
    "        f.write(f\"Source  : {meta_text}\\n\")\n",
    "        f.write(f\"Abstract: {abstract}\\n\")\n",
    "        f.write(\"\\n\"+\"-\"*100+\"\\n\\n\")\n",
    " \n",
    "print(f\"Done Scraping {len(papers)} newest papers for  '{query_input}' .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58522d-9c74-41c9-9d08-d868a37694c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
